/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    mskilab-org/nf-jabba Nextflow config file
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Default config options for all compute environments
----------------------------------------------------------------------------------------
*/

// Global default params, used in configs
params {


	// TODO nf-core: Specify your pipeline's command line flags
	// Input options (Mandatory!)
	input                      = null
	postprocess_bams           = true
    tumor_only                 = false

	// References
	genome                     = 'GATK.GRCh37'
	igenomes_base              = 's3://ngi-igenomes/igenomes'
	igenomes_ignore            = false
	mski_base                  = 's3://mskilab-pipeline'
	save_reference             = true
	build_only_index           = false                            // Only build the reference indexes
	download_cache             = false                            // Do not download annotation cache

	// Options to consider
	// Main options
	no_intervals               = false                            // Intervals will be built from the fasta file
	nucleotides_per_second     = 200000                           // Default interval size
	tools                      = null                             // No default Variant_Calling or Annotation tools
	skip_tools                 = null                             // All tools (markduplicates + baserecalibrator + QC) are used by default
	split_fastq                = 0                                // FASTQ files will not be split by default by FASTP, set to 50000000 for 50M reads/fastq

	// Modify FASTQ files (trim/split) with FASTP
	trim_fastq                 = false                            // No trimming by default
	clip_r1                    = 0
	clip_r2                    = 0
	three_prime_clip_r1        = 0
	three_prime_clip_r2        = 0
	trim_nextseq               = 0
	save_trimmed               = false
	save_split_fastqs          = false

	use_gpu                    = false

	// Alignment
	aligner                    = 'fq2bam'                       // Default is gpu-accelerated fq2bam; bwa-mem, bwa-mem2 and dragmap can be used too
    fq2bam_mark_duplicates     = true                            // Whether fq2bam should mark duplicates, set false if not using fq2bam
    fq2bam_low_memory          = false                           // Set to true if using fq2bam with gpus that have <24GB memory
    optical_duplicate_pixel_distance = 2500                      // For computing optical duplicates, 2500 for NovaSeqX+
	save_mapped                = false                            // Mapped BAMs are saved
	save_output_as_bam         = true                            // Output files from alignment are saved as bam by default and not as cram files
	seq_center                 = null                            // No sequencing center to be written in read group CN field by aligner
	seq_platform               = null                            // Default platform written in read group PL field by aligner, null by default.
	is_run_post_processing     = false                           // Force run bam post processing steps.
	is_run_qc_duplicates       = false                           // Force run estimate library complexity.

	// Structural Variant Calling
	error_rate                 = 0.01                            // Default error_rate for Svaba

	//indel_mask                 = null                            // Must provide blacklist bed file for indels based on genome to run Svaba

	// SV filtering (tumor only)
	pad_junc_filter            = 1000                            // Default Padding for SV Junction filtering
	is_run_junction_filter     = false                           // Run junction filtering steps post-GRIDSS
    parallelize_gridss         = true                            // Run GRIDSS in parallel mode for preprocess and assembly steps

    // AMBER options
    target_bed_amber = null   // pass a bed file with target regions for running AMBER on targeted sequencing sample

    // HetPileups options
    filter_hets      = "TRUE"
    max_depth   = 1000

    // SigprofilerAssignment options
    sigprofilerassignment_cosmic_version = 3.4

	// fragCounter options
	midpoint_frag              = "TRUE"                           // If TRUE only count midpoint if FALSE then count bin footprint of every fragment interval: Default=TRUE
	windowsize_frag            = 1000                            // Window / bin size : Default=200 (but dryclean uses 1000 binsize)
	minmapq_frag               = 20                              // Minimal map quality : Default = 1
	paired_frag                = "TRUE"                           // Is the dataset paired : Default = TRUE
	exome_frag                 = "FALSE"	                        // Use exons as bins instead of fixed window : Default = FALSE

    // Dryclean options
    center_dryclean                    = "TRUE"
    cbs_dryclean                         = "FALSE"
    cnsignif_dryclean                    = 0.00001
    wholeGenome_dryclean                 = "TRUE"
    blacklist_dryclean                   = "FALSE"
    blacklist_path_dryclean              = "NA"
    germline_filter_dryclean             = "FALSE"
    germline_file_dryclean               = "NA"
    human_dryclean                       = "TRUE"
    field_dryclean                       = "reads"

	// ASCAT options
	field_ascat                         = "foreground"
	hets_thresh_ascat                   = 0.2
	penalty_ascat                       = 70
	gc_correct_ascat                    = "TRUE"
	rebin_width_ascat                   = 50000
	from_maf_ascat                      = "FALSE"

    // PURPLE options
    purple_use_svs = "TRUE"
    purple_use_smlvs = "TRUE"
    purple_highly_diploid_percentage = 0.97
    purple_min_purity = 0.08
    purple_ploidy_penalty_factor = 0.4

    // CBS options
    cnsignif_cbs                        = 0.00001
    field_cbs                           = "foreground"
    name_cbs                            = "tumor"

	// JaBbA options
    is_retier_whitelist_junctions = false
    blacklist_junctions_jabba = "NULL"
    geno_jabba = "FALSE"
    indel_jabba = "exclude"
    tfield_jabba = "tier"
    iter_jabba = 2
    rescue_window_jabba = 10000
    rescue_all_jabba = "TRUE"
    nudgebalanced_jabba = "TRUE"
    edgenudge_jabba = 0.1
    strict_jabba = "FALSE"
    allin_jabba = "FALSE"
    field_jabba = "foreground"
    maxna_jabba = 0.9
    ploidy_jabba = "NA"
    purity_jabba = "NA"
    pp_method_jabba = "ppgrid"
    cnsignif_jabba = 0.00001
    slack_jabba = 20
    linear_jabba = "FALSE"
    tilim_jabba = 7200
    epgap_jabba = 0.000001
    fix_thres_jabba = -1
    lp_jabba = "TRUE"
    ism_jabba = "TRUE"
    filter_loose_jabba = "FALSE"
    gurobi_jabba = "FALSE"
    verbose_jabba = "TRUE"

    // Allelic CN (Non-Integer Balance)
	field_non_integer_balance = "foreground"
	hets_thresh_non_integer_balance = 0.2
	overwrite_non_integer_balance = "TRUE"
	lambda_non_integer_balance = 20
	allin_non_integer_balance = "TRUE"
	fix_thresh_non_integer_balance = 10
	nodebounds_non_integer_balance = "TRUE"
	ism_non_integer_balance = "FALSE"
	epgap_non_integer_balance = 0.000001
	tilim_non_integer_balance = 6000
	gurobi_non_integer_balance = "FALSE"
	pad_non_integer_balance = 101

    // Allelic CN (LP-Phased Balance)
	lambda_lp_phased_balance = 100
	cnloh_lp_phased_balance = "TRUE"
	major_lp_phased_balance = "TRUE"
	allin_lp_phased_balance = "TRUE"
	marginal_lp_phased_balance = "TRUE"
	from_maf_lp_phased_balance = "FALSE"
	ism_lp_phased_balance = "FALSE"
	epgap_lp_phased_balance = 0.001
	hets_thresh_lp_phased_balance = 0.2
	min_bins_lp_phased_balance = 3
	min_width_lp_phased_balance = 0
	trelim_lp_phased_balance = 32000
	reward_lp_phased_balance = 10
	nodefileind_lp_phased_balance = 3
	tilim_lp_phased_balance = 6000

    // Oncokb Annotator
    do_vep_oncokb = "TRUE"
    vep_dir_oncokb = ""
    oncokb_api_token = ""

    // HRDetect
    hrdetect_mask = null

    // OnenessTwoness
    model_oneness_twoness = 's3://mskilab-pipeline/onenesstwoness/stash.retrained.model.rds'

    // Variant Calling
    only_paired_variant_calling   = false // if true, skips germline variant calling for normal-paired samples
	ascat_ploidy                  = null  // default value for ASCAT
    ascat_min_base_qual           = 20    // default value for ASCAT
    ascat_min_counts              = 10    // default value for ASCAT
    ascat_min_map_qual            = 35    // default value for ASCAT
    ascat_purity                  = null  // default value for ASCAT
    cf_ploidy                     = "2"   // default value for Control-FREEC
    cf_coeff                      = 0.05  // default value for Control-FREEC
    cf_contamination              = 0     // default value for Control-FREEC
    cf_contamination_adjustment   = false // by default we are not using this in Control-FREEC
    cf_mincov                     = 0     // ControlFreec default values
    cf_minqual                    = 0     // ControlFreec default values
    cf_window                     = null  // by default we are not using this in Control-FREEC
    cnvkit_reference              = null  // by default the reference is build from the fasta file
    concatenate_vcfs              = false // by default we don't concatenate the germline-vcf-files
    ignore_soft_clipped_bases     = false // no --dont-use-soft-clipped-bases for GATK Mutect2
    wes                           = false // Set to true, if data is exome/targeted sequencing data. Used to use correct models in various variant callers
    joint_germline                = false // g.vcf & joint germline calling are not run by default if HaplotypeCaller is selected
    joint_mutect2                = false // if true, enables patient-wise multi-sample somatic variant calling
    sentieon_haplotyper_emit_mode = "variant" // default value for Sentieon haplotyper

    // Annotation
    dbnsfp                    = null // No dbnsfp processed file
    dbnsfp_consequence        = null // No default consequence for dbnsfp plugin
    dbnsfp_fields             = "rs_dbSNP,HGVSc_VEP,HGVSp_VEP,1000Gp3_EAS_AF,1000Gp3_AMR_AF,LRT_score,GERP++_RS,gnomAD_exomes_AF" // Default fields for dbnsfp plugin
    dbnsfp_tbi                = null // No dbnsfp processed file index
    outdir_cache              = null // No default outdir cache
    snpeff_cache              = "s3://mskilab-pipeline/snpeff_cache/"
    spliceai_indel            = null // No spliceai_indel file
    spliceai_indel_tbi        = null // No spliceai_indel file index
    spliceai_snv              = null // No spliceai_snv file
    spliceai_snv_tbi          = null // No spliceai_snv file index
    use_annotation_cache_keys = false
    vep_cache                 = null //'s3://annotation-cache/vep_cache/' but not using it
    vep_custom_args           = "--everything --filter_common --per_gene --total_length --offline --format vcf" // Default arguments for VEP
    vep_dbnsfp                = null // dbnsfp plugin disabled within VEP
    vep_include_fasta         = false // Don't use fasta file for annotation with VEP
    vep_loftee                = null // loftee plugin disabled within VEP
    vep_out_format            = "vcf"
    vep_spliceai              = null // spliceai plugin disabled within VEP
    vep_spliceregion          = null // spliceregion plugin disabled within VEP

	// Signatures
	filter_ffpe_impact        = false // Default is to not run FFPE impact signatures


	// MultiQC options
	multiqc_config             = null
	multiqc_title              = null
	multiqc_logo               = null
	max_multiqc_email_size     = '25.MB'
	multiqc_methods_description = null


	// Boilerplate options
	outdir                     = null
	publish_dir_mode           = 'copy'
	email                      = null
	email_on_fail              = null
	plaintext_email            = false
	monochrome_logs            = false
	hook_url                   = null
	help                       = false
	version                    = false

	// Config options
	config_profile_name        = null
	config_profile_description = null
	custom_config_version      = 'master'
	custom_config_base         = "https://raw.githubusercontent.com/nf-core/configs/${params.custom_config_version}"
	config_profile_contact     = null
	config_profile_url         = null

	// Max resource options
	// Defaults only, expecting to be overwritten
	max_memory                 = 256.GB
	max_cpus                   = 16
	max_time                   = 360.h
    max_accelerator            = 8

	// Schema validation default options
	validationFailUnrecognisedParams = false
	validationLenientMode            = false
	validationSchemaIgnoreParams     = 'genomes, cf_ploidy'
	validationShowHiddenParams       = false
	validate_params                  = false

}

// General configurations
// Order of parameters are sourced/included
// will determine the configuration of the parameters
// at run time.
// Having profiles defined last means that
// these will take precedence over the parameters
// defined in base.config and modules.config.
// Thus the order of precedence (highest priority first):
// profiles > modules.config > base.config

// Load base.config by default for all pipelines
includeConfig 'conf/base.config'



// Load nf-core custom profiles from different Institutions
try {
	includeConfig "${params.custom_config_base}/nfcore_custom.config"
} catch (Exception e) {
	System.err.println("WARNING: Could not load nf-core/config profiles: ${params.custom_config_base}/nfcore_custom.config")
}

// Load mskilab-org/nf-jabba custom profiles from different institutions.
// Warning: Uncomment only if a pipeline-specific instititutional config already exists on nf-core/configs!
// try {
//   includeConfig "${params.custom_config_base}/pipeline/nfjabba.config"
// } catch (Exception e) {
//   System.err.println("WARNING: Could not load nf-core/config/nfjabba profiles: ${params.custom_config_base}/pipeline/nfjabba.config")
// }


// Set default registry for Apptainer, Docker, Podman and Singularity independent of -profile
// Will not be used unless Apptainer / Docker / Podman / Singularity are enabled
// Set to your registry if you have a mirror of containers
apptainer.registry   = 'quay.io'
docker.registry      = 'quay.io'
podman.registry      = 'quay.io'
singularity.registry = 'quay.io'

// Nextflow plugins
plugins {
	id 'nf-validation' // Validation of pipeline parameters and creation of an input channel from a sample sheet
}


// Export these variables to prevent local Python/R libraries from conflicting with those in the container
// The JULIA depot path has been adjusted to a fixed path `/usr/local/share/julia` that needs to be used for packages in the container.
// See https://apeltzer.github.io/post/03-julia-lang-nextflow/ for details on that. Once we have a common agreement on where to keep Julia packages, this is adjustable.

env {
	PYTHONNOUSERSITE = 1
	R_PROFILE_USER   = "/.Rprofile"
	R_ENVIRON_USER   = "/.Renviron"
	JULIA_DEPOT_PATH = "/usr/local/share/julia"
	R_DATATABLE_NUM_THREADS = 1
}

// Capture exit codes from upstream processes when piping
process.shell = ['/bin/bash', '-euo', 'pipefail']

def trace_timestamp = new java.util.Date().format( 'yyyy-MM-dd_HH-mm-ss')
timeline {
	enabled = true
	file    = "${params.outdir}/pipeline_info/execution_timeline_${trace_timestamp}.html"
}
report {
	enabled = true
	file    = "${params.outdir}/pipeline_info/execution_report_${trace_timestamp}.html"
}
trace {
	enabled = true
	file    = "${params.outdir}/pipeline_info/execution_trace_${trace_timestamp}.txt"
}
dag {
	enabled = true
	file    = "${params.outdir}/pipeline_info/pipeline_dag_${trace_timestamp}.html"
}

manifest {
	name            = 'mskilab-org/nf-jabba'
	author          = """Tanubrata Dey and Shihab Dider"""
	homePage        = 'https://github.com/mskilab-org/nf-casereports'
	description     = """Clinical Core Pipeline for MSkiLab"""
	mainScript      = 'main.nf'
	nextflowVersion = '!>=23.04.0'
	version         = '1.0dev'
	doi             = ''
}

// // Load modules.config for output configuration
// includeConfig 'conf/modules.config'

// Function to ensure that resource requirements don't go beyond
// a maximum limit
def check_max(obj, type) {
    if (type == 'memory') {
        try {
            if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                return params.max_memory as nextflow.util.MemoryUnit
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'time') {
        try {
            if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                return params.max_time as nextflow.util.Duration
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'cpus') {
        try {
            return Math.min( obj, params.max_cpus as int )
        } catch (all) {
            println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'accelerator') {
        try {
            return Math.min( obj, params.max_accelerator as int )
        } catch (all) {
            println "   ### ERROR ###   Max gpu '${params.max_accelerator}' is not valid! Using default value: $obj"
            return obj
        }
    }
}

// Must come before includeConfig 'conf/modules.config'
// Otherwise publishDir directives in modules.config 
// not evaluated if defined as directives in profile { process {} }
profiles {
    nyu {
        def singularityModule = "singularity/3.11.5"
        params {
            config_profile_name           = 'mskilab-org NYU cluster'
            config_profile_description    = "mskilab-org Nextflow config for NYU cluster"
            config_profile_contact        = "kevinmhadi@gmail.com"

            cplexDir                      = "/gpfs/data/imielinskilab/Software/CPLEX"
            mski_base                     = "/gpfs/data/imielinskilab/data/nf_gos_files/mskilab_pipeline"
            igenomes_base                 = "/gpfs/data/imielinskilab/data/nf_gos_files/igenomes"
            snpeff_cache                  = "/gpfs/data/imielinskilab/data/nf_gos_files/mskilab_pipeline/snpeff_cache"
			sigprofiler_cache             = "/gpfs/data/imielinskilab/data/nf_gos_files/mskilab_pipeline/sigprofilerassignment"
            vep_dir_oncokb                = "/gpfs/data/imielinskilab/DB/VEP_/"
			use_gpu                       = true // Use GPU for PARABRICKS_FQ2BAM and BAMMETRICS and GPU_ by default

            // Resources
            max_memory = 256.GB
            max_cpus = 48
            max_time = 168.h

            // PARABRICKS_FQ2BAM
            bwa_streams = 2 // Affects GPU memory usage, 1 for 16 GB, 2 for 16-40 GB, 4 for 40-80 GB (a100)
            low_memory_command = "" // Refers to memory on GPU device, not CPU RAM on node
            normalized_queue_capacity = 5 // Affects CPU RAM Usage. 10 is default. 5 is to make sure OOM doesn't occur. Ideally should be divisible by accelerator.request value
        }

        process {
            withName: '.*' {
                errorStrategy = { task.exitStatus in ([130..145] + [104, 155, 255]) ? 'retry' : 'finish' }
                maxRetries    = 100
                maxErrors     = 100
            }

            errorStrategy = { task.exitStatus in ((130..145) + 104) ? 'retry' : 'finish' }
            maxRetries = 50
            maxErrors = 50
            maxAttempts = 50

            containerOptions = null

            withName: '.*FASTQC.*' {
                maxForks = 2
            }

            withName: '.*PARABRICKS_FQ2BAM.*' {
                errorStrategy = "retry"
                accelerator = [request: 1, type: "a100"]
                clusterOptions = "--gres=gpu:a100:${accelerator.request}"
                memory = { check_max( 72.GB * task.attempt, 'memory' ) } // Refers to CPU RAM requested on node
                cpus = 8 // Affects CPU RAM usage. Ideally should be divisible by accelerator.request value
                time = { check_max( 24.h * task.attempt, 'time' ) } 
                // bwa_streams = 2 // Affects GPU memory usage, 1 for 16 GB, 2 for 16-40 GB, 4 for 40-80 GB (a100)
                // low_memory_command = "" // Refers to memory on GPU device, not CPU RAM on node
                // normalized_queue_capacity = 5 // Affects CPU RAM Usage. 10 is default. 5 is to make sure OOM doesn't occur. Ideally should be divisible by accelerator.request value
            }

            withName: 'PARABRICKS_BAMMETRICS' {
                errorStrategy = "retry"
				accelerator = [request: 1, type: "a100"]
                clusterOptions = "--gres=gpu:a100:${accelerator.request}"
                cpus = 12
                memory = { check_max( 72.GB * task.attempt, 'memory' ) }
                time = { check_max( 8.h  * task.attempt, 'time'    ) }
            }

			withName: 'GPU_COLLECTMULTIPLEMETRICS' {
                errorStrategy = "retry"
                maxRetries    = 100
                maxErrors     = 100
				accelerator = [request: 1, type: "a100"]
                clusterOptions = "--gres=gpu:a100:${accelerator.request}"
                time = { check_max( 4.h  * task.attempt, 'time'    ) }
            }

            withName: 'JABBA' {
                errorStrategy = "retry"
                maxRetries    = 100
                maxErrors     = 100
                // Check if CPLEX directory exists
                def cplexExists = new File(params.cplexDir).exists()

                // Conditionally set container options and environment variable
                containerOptions = cplexExists ?
                    "--bind ${params.cplexDir}:/opt/cplex --env PATH=/usr/local/bin/:/opt/cplex/bin/x86-64_linux:\$PATH" : "--env PATH=\$PATH"
            }

            withName: 'NON_INTEGER_BALANCE' {
                errorStrategy = "retry"
                maxRetries    = 100
                maxErrors     = 100
                // Check if CPLEX directory exists
                def cplexExists = new File(params.cplexDir).exists()

                // Conditionally set container options and environment variable
                containerOptions = cplexExists ?
                    "--bind ${params.cplexDir}:/opt/cplex --env PATH=/usr/local/bin/:/opt/cplex/bin/x86-64_linux:\$PATH" : "--env PATH=\$PATH"
            }

            withName: '.*LP_PHASED_BALANCE.*' {
                errorStrategy = "retry"
                maxRetries    = 100
                maxErrors     = 100
                // Check if CPLEX directory exists
                def cplexExists = new File(params.cplexDir).exists()

                // Conditionally set container options and environment variable
                containerOptions = cplexExists ?
                    "--bind ${params.cplexDir}:/opt/cplex --env PATH=/usr/local/bin/:/opt/cplex/bin/x86-64_linux:\$PATH" : "--env PATH=\$PATH"
            }


            // default SLURM node config
            beforeScript = """
                module load $singularityModule
                module load java
                module load cuda
                module load aws-cli
            """
        }

        executor {
            name = 'slurm'
            queue = 'imielinskilab,a100_long,gpu4_long,cpu_long' // Doesn't seem to be recognized in a profile directive.
            queueSize = 500
        }
        // 

        singularity {
            enabled = true
            cacheDir = "/gpfs/data/imielinskilab/data/pipeline/container_images_cache"
        }

    }

    local {

        errorStrategy = { task.exitStatus in ((130..145) + 104) ? 'retry' : 'finish' }

		def singularityModule = "singularity/3.11.5"

		max_memory = 500.GB
		max_cpus = 192
		max_time = 240.h

		process {

            withName: '.*' {
                time   = { check_max( 72.h  * task.attempt, 'time'    ) }
            }

			
        //     containerOptions = null
			

            withName: '.*PARABRICKS_FQ2BAM.*' {
				maxForks = 2
                accelerator = [request: 2, type: "l40s"]
                clusterOptions = "--gres=gpu:l40s:${accelerator.request}"
                cpus   = { check_max( 16    * task.attempt, 'cpus'    ) }
                memory = { check_max( 120.GB * task.attempt, 'memory'  ) } // memory is set to 0.4 * this value in parabricks fq2bam for slurm. This will allow the process to set 48 GB inside the pipeline when running locally.
                time   = { check_max( 24.h  * task.attempt, 'time'    ) }
                bwa_streams = 2
                low_memory_command = ""
                normalized_queue_capacity = 10
            }
            

			withName: '.*FASTQC.*' {
				maxForks = 10
            }

			withName: '.*FASTP.*' {
				maxForks = 10
            }

			withName: '.*PARABRICKS_BAMMETRICS.*' {
				maxForks = 2
            }

			withName: 'GPU_COLLECTMULTIPLEMETRICS' {
				maxForks = 2
            }

            withName: 'GRIDSS_ASSEMBLE_SCATTER' {
				maxForks = 8
            }


            // default SLURM node config
            beforeScript = """
                module load $singularityModule
                module load java
                module load cuda
                module load aws-cli
            """
        }

        executor { 
			name = 'local'
			queueSize = 30
		}
    }


    nygc {
        def singularityModule = "singularity/4.2.2"

        params {
            config_profile_name           = 'mskilab-org NYGC cluster'
            config_profile_description    = "mskilab-org Nextflow config for NYGC cluster"
            config_profile_contact        = "sdider@nygenome.org"

            cplexDir                      = "/gpfs/commons/groups/imielinski_lab/Software/CPLEX/CPLEX_Studio"
            mski_base                     = "/gpfs/commons/groups/imielinski_lab/data/nf_gos_files/mskilab_pipeline"
            igenomes_base                 = "/gpfs/commons/groups/imielinski_lab/data/nf_gos_files/igenomes"
            aligner                       = "bwa-mem2"
            snpeff_cache                  = "/gpfs/commons/groups/imielinski_lab/data/nf_gos_files/mskilab_pipeline/snpeff_cache"
            vep_dir_oncokb                = "/gpfs/commons/groups/imielinski_lab/DB/VEP/"

            // Resources
            max_memory = 500.GB
            max_cpus = 256
            max_time = 240.h
        }


        process {

            withName: 'PARABRICKS_FQ2BAM' {
                queue           =   'gpu'
                clusterOptions = '--gres=gpu:1'
            }

            containerOptions = null

            withName: 'JABBA' {
                // Check if CPLEX directory exists
                def cplexExists = new File(params.cplexDir).exists()

                // Conditionally set container options and environment variable
                containerOptions = cplexExists ?
                    "--bind ${params.cplexDir}:/opt/cplex --env PATH=/opt/cplex/bin/x86-64_linux:\$PATH" : "--env PATH=\$PATH"
            }

            withName: 'NON_INTEGER_BALANCE' {
                // Check if CPLEX directory exists
                def cplexExists = new File(params.cplexDir).exists()

                // Conditionally set container options and environment variable
                containerOptions = cplexExists ?
                    "--bind ${params.cplexDir}:/opt/cplex --env PATH=/opt/cplex/bin/x86-64_linux:\$PATH" : "--env PATH=\$PATH"
            }

            withName: 'LP_PHASED_BALANCE' {
                // Check if CPLEX directory exists
                def cplexExists = new File(params.cplexDir).exists()

                // Conditionally set container options and environment variable
                containerOptions = cplexExists ?
                    "--bind ${params.cplexDir}:/opt/cplex --env PATH=/opt/cplex/bin/x86-64_linux:\$PATH" : "--env PATH=\$PATH"
            }

            // default SLURM node config
            beforeScript = """
		        module unload java
                module load $singularityModule
                module load Java/15
                module load cuda
            """

            executor='slurm'

            // memory errors which should be retried. otherwise error out
            errorStrategy = { task.exitStatus in ((130..145) + 104) ? 'retry' : 'finish' }
            maxRetries    = 50
            // maxErrors     = '-1'
            maxErrors     = 50

        }

        executor {
            name = 'slurm'
            queue = 'ne12'
            queueSize = 500
        }

        singularity {
            enabled = true
            cacheDir = "/gpfs/commons/groups/imielinski_lab/data/pipeline/container_images_cache"
        }
    }

    chr21_test {
        process {
            withName: 'GRIDSS_GRIDSS' {
                cpus            = { check_max( 4 * task.attempt, 'cpus' ) }
                memory          = { check_max( 32.GB * task.attempt, 'memory' ) }
                time            = { check_max( 2.h  * task.attempt, 'time'    ) }
            }
            withName: 'GRIDSS_SOMATIC' {
                cpus            = { check_max( 2 * task.attempt, 'cpus' ) }
                memory          = { check_max( 8.GB * task.attempt, 'memory' ) }
                time            = { check_max( 1.h  * task.attempt, 'time'    ) }
            }
            withName: 'SAGE_SOMATIC' {
                cpus            = { check_max( 20 * task.attempt, 'cpus' ) }
                memory          = { check_max( 16.GB * task.attempt, 'memory' ) }
                time            = { check_max( 1.h  * task.attempt, 'time'    ) }
            }
            withName: 'SAGE_GERMLINE' {
                cpus            = { check_max( 8 * task.attempt, 'cpus' ) }
                memory          = { check_max( 16.GB * task.attempt, 'memory' ) }
                time            = { check_max( 1.h  * task.attempt, 'time'    ) }
            }
            withName: 'JABBA' {
                cpus            = { check_max( 4 * task.attempt, 'cpus' ) }
                memory          = { check_max( 16.GB * task.attempt, 'memory' ) }
                time            = { check_max( 1.h  * task.attempt, 'time'    ) }
            }
            withName: 'FUSIONS' {
                cpus            = { check_max( 4 * task.attempt, 'cpus' ) }
                memory          = { check_max( 16.GB * task.attempt, 'memory' ) }
                time            = { check_max( 1.h  * task.attempt, 'time'    ) }
            }
        }
    }

	debug {
		dumpHashes             = true
		process.beforeScript   = 'echo $HOSTNAME'
        	cleanup                = false
    	}
	conda {
		conda.enabled          = true
		docker.enabled         = false
		singularity.enabled    = false
		podman.enabled         = false
		shifter.enabled        = false
		charliecloud.enabled   = false
		apptainer.enabled      = false
	}
	mamba {
		conda.enabled          = true
		conda.useMamba         = true
		docker.enabled         = false
		singularity.enabled    = false
		podman.enabled         = false
		shifter.enabled        = false
		charliecloud.enabled   = false
		apptainer.enabled      = false
	}
	docker {
		docker.enabled         = true
		docker.userEmulation   = true
		conda.enabled          = false
		singularity.enabled    = false
		podman.enabled         = false
		shifter.enabled        = false
		charliecloud.enabled   = false
		apptainer.enabled      = false
	}
	arm {
		docker.runOptions = '-u $(id -u):$(id -g) --platform=linux/amd64'
	}
	singularity {
		singularity.enabled    = true
		singularity.autoMounts = true
        singularity.runOptions = '--env CPLEX_DIR=/opt/cplex --env R_DATATABLE_NUM_THREADS=1 --bind /usr/lib/locale'
		conda.enabled          = false
		docker.enabled         = false
		podman.enabled         = false
		shifter.enabled        = false
		charliecloud.enabled   = false
		apptainer.enabled      = false
	}
	podman {
		podman.enabled         = true
		conda.enabled          = false
		docker.enabled         = false
		singularity.enabled    = false
		shifter.enabled        = false
		charliecloud.enabled   = false
		apptainer.enabled      = false
	}
	shifter {
		shifter.enabled        = true
		conda.enabled          = false
		docker.enabled         = false
		singularity.enabled    = false
		podman.enabled         = false
		charliecloud.enabled   = false
		apptainer.enabled      = false
	}
	charliecloud {
		charliecloud.enabled   = true
		conda.enabled          = false
		docker.enabled         = false
		singularity.enabled    = false
		podman.enabled         = false
		shifter.enabled        = false
		apptainer.enabled      = false
	}
	apptainer {
		apptainer.enabled      = true
		conda.enabled          = false
		docker.enabled         = false
		singularity.enabled    = false
		podman.enabled         = false
		shifter.enabled        = false
		charliecloud.enabled   = false
	}
	gitpod {
		executor.name          = 'local'
		executor.cpus          = 16
		executor.memory        = 60.GB
	}

	//basic test config files
	test      { includeConfig 'conf/test.config'      }
	test_full { includeConfig 'conf/test_full.config' }

}

// Load igenomes.config if required
if (!params.igenomes_ignore) {
	includeConfig 'conf/igenomes.config'
} else {
	params.genomes = [:]
}

// Load modules.config for output configuration
includeConfig 'conf/modules.config'